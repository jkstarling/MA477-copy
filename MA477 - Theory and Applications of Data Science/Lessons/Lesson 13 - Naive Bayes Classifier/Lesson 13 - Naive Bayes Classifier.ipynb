{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ======================================================</h2>\n",
    " <h1>MA477 - Theory and Applications of Data Science</h1> \n",
    "  <h1>Lesson 13: Naive Bayes Classifier (NBC) </h1> \n",
    " \n",
    " <h4>Dr. Valmir Bucaj</h4>\n",
    " United States Military Academy, West Point \n",
    "AY20-2\n",
    "<h2>======================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lecture Outline</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>What is NBC?</li>\n",
    "    <li> What is NBC used for?</li>\n",
    "    <li>Bayes Theorem</li>\n",
    "    <li> Applications of NBC</li>\n",
    "    <li> Natural Language Tool Kit (NLTK) Overview</li>\n",
    "    <li> Text Counting</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "<h3>What is NBC?</h3>\n",
    "\n",
    "Naive Bayes classifier, as the name suggests, is a supervised machine-learning technique that is used in classification problems. At the heart of NBC is Bayes Theorem(which will be discussed shortly), which is used to calculate the probability that a new data point will belong to a certain class. \n",
    "\n",
    "<h3>What is NBC used for?</h3>\n",
    "\n",
    "NBC can be used to classify data. In this course, we will use NBC to classify <b>text</b> and perform <b> sentiment analysis</b>.\n",
    "\n",
    "Examples include:\n",
    "\n",
    "<ul>\n",
    "    \n",
    " <li> Natural Language Processing (NLP)</li>\n",
    " <li> Email Spam Detection</li>\n",
    " <li>Classify whether a name is male, female, gender-neutral etc.</li>\n",
    " <li> Classify whether product reviews are good or bad </li>\n",
    " <li> etc.</li>\n",
    " </ul>\n",
    " \n",
    " Before we describe a concrete applicaton of NBC that we will do later on, let us briefly turn to the Bayes Theorem.\n",
    " \n",
    " <h3>Bayes Theorem</h3>\n",
    " \n",
    " Let $A,B$ be any two events with $P(B)\\neq 0.$ Then,\n",
    " \n",
    " $$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$ where $P(B)=P(B|A)P(A)+P(B|A^c)P(A^c)$\n",
    " \n",
    " <h3>Application of NBC</h3>\n",
    " \n",
    " Suppose we want to classify a name as a female or male name. For example,\n",
    " \n",
    " Marie, Ana, Sophia, Zoey are typical female names, while John, Karl, Brad etc. are typical male names. \n",
    " \n",
    " First off we would need a strategy and a criterion to help us make this distinction. One such simple criteria could be the letter that the name ends with. Typically, female names end in vowels A, E, I, O, U, Y and male names typically end in consonants. \n",
    " \n",
    " So, in our case we have two classes $C_1=Female$ and $C_2=Male$, while the target that we want to classify will be written as a vector $\\vec{x}=(a,e,i,o,u,y)$.\n",
    " \n",
    " Our goal is to find $$P(C_i|\\vec{x})$$ for $i=1,2.$ In other words, given that a name ends in a vowel, we want to compute the probability that it is a female name and the probability that it is a male name (which in this case it is simply the complement of the prob. of being a female name since we are only dealing with two classes). We make our decision based on which of these two probabilities is largest.\n",
    " \n",
    "This is where Bayes Theorem comes in handy. Instead of computing the above probabilities directly, we do it using Bayes Theorem, namely\n",
    "\n",
    "\n",
    "$$P(C_i|\\vec{x})=\\frac{P(\\vec{x}|C_i)P(C_i)}{P(\\vec{x})}$$\n",
    "\n",
    "One thing to point out immediately is that we will completely ignore the denominator, as it is common to all the classes, and we are not interested in computing the exact probability, but rather how they compare with one another. Here, $P(C_i)$ is known as the prior probability; that is, we need to have some apriory sense of the probability for a name to be male or female, before knowing anything else about the particular name in question. Wheras the probability that a name ends in a vowel given that it is male or female, $P(\\vec{x}|C_i)$, is obtained empiricaly from the data that we have; that is, we need to have a collection of names that we know whether they are male or female, which we will use to train our model. \n",
    "\n",
    "So, why the term $Naive?$ It's called <b> Naive</b> Bayes Classifier, because of the assumption that the features are indepndent of each other. \n",
    "\n",
    "\n",
    " <h3>NLTK</h3>\n",
    " \n",
    " NLTK is a collection of Python libraries which are used to conduct symbolic and statistical natural language processing. For more information visit www.nltk.org\n",
    " \n",
    "We begin by importing this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside ntlk is a wide corpus of digitized books and other texts, which we will often use as examples to illustrate certain points. Let's go ahead and import some text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\valmir.bucaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\valmir.bucaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, these are all the texts that are contained in the nltk corpus as part of the gutenberg project. We'll use the 'carroll-alice' to illustrate text counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice=nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can check `alice` contains a list of all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Alice',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " '1865',\n",
       " ']',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " '.',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit',\n",
       " '-',\n",
       " 'Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to check how many times the words `Alice`, `Rabbit` etc. appears in the text we may do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice.count('Alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice.count('Rabbit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice.count('rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, it is case sensitive.\n",
    "\n",
    "We can also check how many words/items are total in the text, to get a rough idea of its length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34110"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check how many unique words/items are in the text, we can first convert the list to a set and then compute its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_set=set(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3016"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is a little over 3000 unique words/items in the text. Now, we can check the average number of times each item/word appears in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.309681697612731"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice)/len(alice_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, on average, each word/item appears roughly 11 times in the text.\n",
    "\n",
    "Instead of getting the individual words out of the text, we may instead get the sentences, so that we may begin to get a better idea of the overall structure of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_sent=nltk.corpus.gutenberg.sents('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will soon check, `alice_sent` is a list of lists, where each list-element is a sentence broken-down into individual words. For example, below is what the fifth sentence looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'so',\n",
       " 'VERY',\n",
       " 'remarkable',\n",
       " 'in',\n",
       " 'that',\n",
       " ';',\n",
       " 'nor',\n",
       " 'did',\n",
       " 'Alice',\n",
       " 'think',\n",
       " 'it',\n",
       " 'so',\n",
       " 'VERY',\n",
       " 'much',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'the',\n",
       " 'Rabbit',\n",
       " 'say',\n",
       " 'to',\n",
       " 'itself',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Oh',\n",
       " 'dear',\n",
       " '!']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_sent[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the average number of words per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.029359953024077"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice)/len(alice_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a sentence contains on average about 20 words.\n",
    "\n",
    "<font color='red' size='4'>Exercise</font>\n",
    "\n",
    "Load the inagural speeches and compute the average number of words per sentence in each speech. (Later we'll do more interesting things with the inagural speeches!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\valmir.bucaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches=inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Your Answer Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Frequency Distributions</h3>\n",
    "\n",
    "When analyzing text, one of the most basic things we may want to know if how often a list of words appears in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_fd=nltk.FreqDist(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 1993, \"'\": 1731, 'the': 1527, 'and': 802, '.': 764, 'to': 725, 'a': 615, 'I': 543, 'it': 527, 'she': 509, ...})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `alice_fd` is a dictionary that contains the number of times each item/word appears in the text. For example, if I want to know how many times the word `dear` appears we can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fd['dear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great feature of the Frequency Distribution method is that we can use it to return the most common words/items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1993),\n",
       " (\"'\", 1731),\n",
       " ('the', 1527),\n",
       " ('and', 802),\n",
       " ('.', 764),\n",
       " ('to', 725),\n",
       " ('a', 615),\n",
       " ('I', 543),\n",
       " ('it', 527),\n",
       " ('she', 509),\n",
       " ('of', 500),\n",
       " ('said', 456),\n",
       " (\",'\", 397),\n",
       " ('Alice', 396),\n",
       " ('in', 357)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fd.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the character `comma` is the one that appears the most, followed by the aphostrophe and the word `the`. \n",
    "\n",
    "Is this list informatory at all? Not so much! Other than `Alice` all other words/items tell us absolutely nothing about the actual content of the text. In the future we will learn how to first get rid of these non-descriptive words, so that we can gain a better idea of the actual contents of the text.\n",
    "\n",
    "We can also check the words that were used only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lewis',\n",
       " 'Carroll',\n",
       " '1865',\n",
       " ']',\n",
       " 'Hole',\n",
       " 'conversations',\n",
       " 'daisy',\n",
       " 'chain',\n",
       " 'daisies',\n",
       " 'pink']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fd.hapaxes()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Conditional Frequency Distributions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
