{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ======================================================</h2>\n",
    " <h1>MA477 - Theory and Applications of Data Science</h1> \n",
    "  <h1>Lesson 8: Cross-Validation </h1> \n",
    " \n",
    " <h4>Dr. Valmir Bucaj</h4>\n",
    " United States Military Academy, West Point \n",
    "AY20-2\n",
    "<h2>======================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lecture Outline</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>What is Cross-Validation?</li>\n",
    "    <li> Validation Set Method</li>\n",
    "    <li>Leave-One-Out Cross-Validation (LOOCV)</li>\n",
    "    <li>$k-$Fold Cross-Validation</li>\n",
    "    <li>Bias-Variance Trade-Off for k-Fold CV</li>\n",
    "    <li>Implementing Cross-Validation with Python</li>\n",
    " \n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> What is Cross-Validation?</h3>\n",
    "\n",
    "Cross-Validation (short CV) is a <i> resampling method </i> most commonly used for <i> model assesment</i>; that is, to evaluate a model's performance via estimating the test error associated with the respective machine-learning method.\n",
    "\n",
    "For example, in order to gain an idea of the variability of our model, what one may want to do is repeatedly draw different samples from the training data, fit the machine-learning model to each of the drawn samples, and compute some metric to examine the extent to which each of the fitted models differ. This kind of insight is impossible to be gained if we only fit once our model to the training data. \n",
    "\n",
    "<h3>Validation Set Method</h3>\n",
    "\n",
    "Recall that when assessing the performance of a machine-learning model we are interested in assessing how well our model performs in making predictions on the new data, previously unseen by the model. In other words, we want to estimate the <i> test error rate</i>. \n",
    "\n",
    "The <i> validation set approach</i> is most appropriate when we have a large dataset and we can afford to split it into a <i> training set</i>(used to train our model) and a <i> test set</i> or a <i> hold-out set</i> which has not been seen by the model before and will be used to compute the <i> test error rate</i>, such as $R^2$ score and $MSE$ in the regression setting.\n",
    "\n",
    "There are two points one has to keep in mind when using the <i> validation set approach</i>:\n",
    "<ul>\n",
    "    <li> The estimates of the test error rates obtained via using the validation set may have a high variance depending on what points are included in the training set</li>\n",
    "    <li> May result in an overestimate of the test error rates due to the fact that machine-learning algorithms tend to perform better with larger training sets</li>\n",
    "    </ul>\n",
    "    \n",
    "We have already implemented the validation set approache when we discussed KNN Regressor and Linear Regression.\n",
    "\n",
    "<h3> Leave-One-Out Cross-Validation</h3>\n",
    "\n",
    "LOOCV is very similar to the <i>validation set approach</i> in the sense that it also involves splitting the dataset in two parts. Despite the similarities, it attempts to overcome the two drawbacks that the validation set approach has, namely, the high variance due to the random split into training and test sets, and the potential to overestimate the test error. \n",
    "\n",
    "Suppose we have $n$ data points $(x_1,y_1),\\dots, (x_n,y_n)$. LOOCV splits the dataset into a single-element validation set and a training set which contains the rest of the data. Specifically, on the first iteration, only the data point $(x_1,y_1)$ will be designated as the validation set and the remaining $n-1$ point $(x_2,y_2),\\dots, (x_n,y_n)$ will be used to train the model. Once the model has been trained, and a prediciton $\\hat y_1$ is made using the excluded observation $x_1$ one computes $MSE_1=(y_1-\\hat{y_1})^2$ to obtain an estimate of the test $MSE$.\n",
    "\n",
    "For obvious reasons, this estimate is poor as it depents on a single point and thus suffers from high-variance. To get around this drawback, we repeat the process by iteratively designating each of $(x_i,y_i)$ as a validation point and using the remaining $n-1$ points $(x_1,y_1),\\dots,(x_{i-1},y_{i-1}),(x_{i+1},y_{i+1}),\\dots,(x_n,y_n)$ to train the model. After the model has been trained, a prediction $\\hat{y_i}$ is made using the point $x_i$ which has not been seen by the model previously and we compute $MSE_i=(y_i-\\hat{y_i})^2$ and average them over the $n$ points to obtain a more roboust estimate of the test $MSE$:\n",
    "\n",
    "$$CV_{(n)}=\\frac{1}{n}\\sum_{i=1}^nMSE_i$$\n",
    "\n",
    "Let's discuss how LOOCV gets around the two drawbacks that the validation set approach suffers from. First off, because there is no random split of the dataset $CV_{(n)}$ will always be the same regardless how many times the model is run. Second, because each time the model is fit, it essentially uses the entire original dataset, it has less tendency to overestimate the test error rate compared to the validation set approach.\n",
    "\n",
    "However, LOOCV does suffer frome drawbacks. Maybe the major one is the fact that it is computationally costly especially if $n$ is large and the machine-learning models we are fitting are complex and take a long time. Which leads us to the next validation method.\n",
    "\n",
    "<h3>k-Fold Cross-Validation</h3>\n",
    "\n",
    "$k-$fold CV is essentially a generalization of the LOOCV. In this case the dataset is randomly split into $k$ approximately equal size subsets. Then, iteratively, one of these subsets (folds) is designated as a validation set and the remaining $k-1$ subsets(folds) are used to fit the model. Once the model is fit, predictions are made using the designated validation set and the mean square error $MSE_i\\approx \\lfloor\\frac{k}{n}\\rfloor\\sum_{j=1}^{\\lfloor\\frac{n}{k}\\rfloor}(y_{ij}-\\hat{y_{ij}})^2$ is computed for $i=1,2,\\dots,k$. Finally, the $k-$fold CV estimate of the test $MSE$ is the average of these values:\n",
    "\n",
    "$$CV_{(k)}=\\frac{1}{k}\\sum_{i=1}^kMSE_i$$\n",
    "\n",
    "As we mentioned, when $k=1$ then $k-$fold CV is simply LOOCV.\n",
    "\n",
    "In contract to LOOCV the $k-$fold CV is computationaly less expensive because instead of fitting the model $n$ times, it only requires fitting it $k$ times, where $k$ is typically taken to be $3,5$ or $10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementing Cross-Validation in Python</h2>\n",
    "\n",
    "Go ahead and import the standard libraries and the `Boston` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"Boston_Dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>RM</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>No</td>\n",
       "      <td>6.575</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>No</td>\n",
       "      <td>6.421</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>No</td>\n",
       "      <td>7.185</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>No</td>\n",
       "      <td>6.998</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>No</td>\n",
       "      <td>7.147</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     CRIM CHAS     RM     DIS  RAD  TAX  PTRATIO  LSTAT  Price\n",
       "0           0  0.00632   No  6.575  4.0900    1  296     15.3   4.98   24.0\n",
       "1           1  0.02731   No  6.421  4.9671    2  242     17.8   9.14   21.6\n",
       "2           2  0.02729   No  7.185  4.9671    2  242     17.8   4.03   34.7\n",
       "3           3  0.03237   No  6.998  6.0622    3  222     18.7   2.94   33.4\n",
       "4           4  0.06905   No  7.147  6.0622    3  222     18.7   5.33   36.2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Pre-processing</h3>\n",
    "\n",
    "<font size=4 color='red'>Exercise</font>\n",
    "\n",
    "  Scale the data, but not the response variable, `PRICE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Price',axis=1)\n",
    "y=df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>RM</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>No</td>\n",
       "      <td>6.575</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>No</td>\n",
       "      <td>6.421</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>No</td>\n",
       "      <td>7.185</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>No</td>\n",
       "      <td>6.998</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>No</td>\n",
       "      <td>7.147</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     CRIM CHAS     RM     DIS  RAD  TAX  PTRATIO  LSTAT\n",
       "0           0  0.00632   No  6.575  4.0900    1  296     15.3   4.98\n",
       "1           1  0.02731   No  6.421  4.9671    2  242     17.8   9.14\n",
       "2           2  0.02729   No  7.185  4.9671    2  242     17.8   4.03\n",
       "3           3  0.03237   No  6.998  6.0622    3  222     18.7   2.94\n",
       "4           4  0.06905   No  7.147  6.0622    3  222     18.7   5.33"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['CHAS']=X['CHAS'].apply(lambda x: 1 if x=='Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc=pd.DataFrame(scaled,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>RM</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.728631</td>\n",
       "      <td>-0.419782</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>-1.075562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.721785</td>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>-0.492439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.714939</td>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>-1.208727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.708093</td>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>-1.361517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.701247</td>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>-1.026501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      CRIM      CHAS        RM       DIS       RAD       TAX  \\\n",
       "0   -1.728631 -0.419782 -0.272599  0.413672  0.140214 -0.982843 -0.666608   \n",
       "1   -1.721785 -0.417339 -0.272599  0.194274  0.557160 -0.867883 -0.987329   \n",
       "2   -1.714939 -0.417342 -0.272599  1.282714  0.557160 -0.867883 -0.987329   \n",
       "3   -1.708093 -0.416750 -0.272599  1.016303  1.077737 -0.752922 -1.106115   \n",
       "4   -1.701247 -0.412482 -0.272599  1.228577  1.077737 -0.752922 -1.106115   \n",
       "\n",
       "    PTRATIO     LSTAT  \n",
       "0 -1.459000 -1.075562  \n",
       "1 -0.303094 -0.492439  \n",
       "2 -0.303094 -1.208727  \n",
       "3  0.113032 -1.361517  \n",
       "4  0.113032 -1.026501  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to implemnet CV is to use the `cross_validate()` method. Below we import this method along with KNN Regressor and Linear Regression which we will use to fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, cross_val_score, ShuffleSplit,  KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg=LinearRegression()\n",
    "knn=KNeighborsRegressor(n_neighbors=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing CV we need to provide an estimator (eg. Linear Regression, KNN etc.) Below, we first implemnet CV with LinearRegression as an estimator and then we do the same thing using KNN Regressor and compare the results. WE also need to provide a way to measure the predictions, such as R2 score, MSE etc.\n",
    "\n",
    "One very important thing to keep in mind is that when applying CV you need to make sure you are shuffling the indicies that are used to split the dataset. We will demonstrate this concept below so that it is clear what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lg=cross_validate(lg,X_sc,y,cv=3,scoring={'r2'},return_estimator=True,return_train_score=True)\n",
    "cv_knn=cross_validate(knn,X_sc,y,cv=3,scoring={'r2'},return_estimator=True,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'estimator', 'test_r2', 'train_r2'])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_lg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34337723,  0.46592017, -0.28248462])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_lg['test_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37601945, 0.13763878, 0.0729882 ])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_knn['test_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=50, p=2,\n",
       "                     weights='uniform'),\n",
       " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=50, p=2,\n",
       "                     weights='uniform'),\n",
       " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=50, p=2,\n",
       "                     weights='uniform'))"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_knn['estimator']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=4>STOP & Reflect</font>\n",
    "\n",
    "As we can see, the $R2$ scores are very low. We worked with this ame set last lecture and were getting better results. This is a great opportunity to stop and reflect about what could possibly be going on??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Shuffling First</h4>\n",
    "\n",
    "Now, let's demonstrate the significant change in the computed metrics once we shuffle the indices frist and then carry out the split. Think about why this is important and why you should always do this first.\n",
    "\n",
    "Below we will demonstrate two differnet ways we may achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=10,random_state=2,shuffle=True)\n",
    "ss=ShuffleSplit(n_splits=10,test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lg=cross_validate(lg,X_sc,y,cv=ss,scoring={'r2'},return_estimator=True,return_train_score=True)\n",
    "cv_knn=cross_validate(knn,X_sc,y,cv=ss,scoring={'r2'},return_estimator=True,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6811674465843506"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_lg['test_r2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7525084661567873"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_knn['test_r2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train, test in kf.split(X_sc):\n",
    "#     print('Train:',train)\n",
    "#     print(\"Test\",test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size='5'>Exercise</font>\n",
    "\n",
    "Load the `diabetes` dataset from `sklearn.datasets`. Designate $20\\%$ of the data as `Validation` data. Use the remaining data to train the model using either KNN Regressor or Linear Regresson to predict the disease progression one year after baseline. Once you have trained the model, compute the test R2 score using the Validation set to get an assessment of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabet=load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - Age\n",
      "      - Sex\n",
      "      - Body mass index\n",
      "      - Average blood pressure\n",
      "      - S1\n",
      "      - S2\n",
      "      - S3\n",
      "      - S4\n",
      "      - S5\n",
      "      - S6\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(diabet.DESCR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
