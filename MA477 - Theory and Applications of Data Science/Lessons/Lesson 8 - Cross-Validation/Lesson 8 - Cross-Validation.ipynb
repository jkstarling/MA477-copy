{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ======================================================</h2>\n",
    " <h1>MA477 - Theory and Applications of Data Science</h1> \n",
    "  <h1>Lesson 8: Cross-Validation </h1> \n",
    " \n",
    " <h4>Dr. Valmir Bucaj</h4>\n",
    " United States Military Academy, West Point \n",
    "AY20-2\n",
    "<h2>======================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lecture Outline</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>What is Cross-Validation?</li>\n",
    "    <li> Validation Set Method</li>\n",
    "    <li>Leave-One-Out Cross-Validation (LOOCV)</li>\n",
    "    <li>$k-$Fold Cross-Validation</li>\n",
    "    <li>Bias-Variance Trade-Off for k-Fold CV</li>\n",
    " \n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> What is Cross-Validation?</h3>\n",
    "\n",
    "Cross-Validation (short CV) is a <i> resampling method </i> most commonly used for <i> model assesment</i>; that is, to evaluate a model's performance via estimating the test error associated with the respective machine-learning method.\n",
    "\n",
    "For example, in order to gain an idea of the variability of our model, what one may want to do is repeatedly draw different samples from the training data, fit the machine-learning model to each of the drawn samples, and compute some metric to examine the extent to which each of the fitted models differ. This kind of insight is impossible to be gained if we only fit once our model to the training data. \n",
    "\n",
    "<h3>Validation Set Method</h3>\n",
    "\n",
    "Recall that when assessing the performance of a machine-learning model we are interested in assessing how well our model performs in making predictions on the new data, previously unseen by the model. In other words, we want to estimate the <i> test error rate</i>. \n",
    "\n",
    "The <i> validation set approach</i> is most appropriate when we have a large dataset and we can afford to split it into a <i> training set</i>(used to train our model) and a <i> test set</i> or a <i> hold-out set</i> which has not been seen by the model before and will be used to compute the <i> test error rate</i>, such as $R^2$ score and $MSE$ in the regression setting.\n",
    "\n",
    "There are two points one has to keep in mind when using the <i> validation set approach</i>:\n",
    "<ul>\n",
    "    <li> The estimates of the test error rates obtained via using the validation set may have a high variance depending on what points are included in the training set</li>\n",
    "    <li> May result in an overestimate of the test error rates due to the fact that machine-learning algorithms tend to perform better with larger training sets</li>\n",
    "    </ul>\n",
    "    \n",
    "We have already implemented the validation set approache when we discussed KNN Regressor and Linear Regression.\n",
    "\n",
    "<h3> Leave-One-Out Cross-Validation</h3>\n",
    "\n",
    "LOOCV is very similar to the <i>validation set approach</i> in the sense that it also involves splitting the dataset in two parts. Despite the similarities, it attempts to overcome the two drawbacks that the validation set approach has, namely, the high variance due to the random split into training and test sets, and the potential to overestimate the test error. \n",
    "\n",
    "Suppose we have $n$ data points $(x_1,y_1),\\dots, (x_n,y_n)$. LOOCV splits the dataset into a single-element validation set and a training set which contains the rest of the data. Specifically, on the first iteration, only the data point $(x_1,y_1)$ will be designated as the validation set and the remaining $n-1$ point $(x_2,y_2),\\dots, (x_n,y_n)$ will be used to train the model. Once the model has been trained, and a prediciton $\\hat y_1$ is made using the excluded observation $x_1$ one computes $MSE_1=(y_1-\\hat{y_1})^2$ to obtain an estimate of the test $MSE$.\n",
    "\n",
    "For obvious reasons, this estimate is poor as it depents on a single point and thus suffers from high-variance. To get around this drawback, we repeat the process by iteratively designating each of $(x_i,y_i)$ as a validation point and using the remaining $n-1$ points $(x_1,y_1),\\dots,(x_{i-1},y_{i-1}),(x_{i+1},y_{i+1}),\\dots,(x_n,y_n)$ to train the model. After the model has been trained, a prediction $\\hat{y_i}$ is made using the point $x_i$ which has not been seen by the model previously and we compute $MSE_i=(y_i-\\hat{y_i})^2$ and average them over the $n$ points to obtain a more roboust estimate of the test $MSE$:\n",
    "\n",
    "$$CV_{(n)}=\\frac{1}{n}\\sum_{i=1}^nMSE_i$$\n",
    "\n",
    "Let's discuss how LOOCV gets around the two drawbacks that the validation set approach suffers from. First off, because there is no random split of the dataset $CV_{(n)}$ will always be the same regardless how many times the model is run. Second, because each time the model is fit, it essentially uses the entire original dataset, it has less tendency to overestimate the test error rate compared to the validation set approach.\n",
    "\n",
    "However, LOOCV does suffer frome drawbacks. Maybe the major one is the fact that it is computationally costly especially if $n$ is large and the machine-learning models we are fitting are complex and take a long time. Which leads us to the next validation method.\n",
    "\n",
    "<h3>k-Fold Cross-Validation</h3>\n",
    "\n",
    "$k-$fold CV is essentially a generalization of the LOOCV. In this case the dataset is randomly split into $k$ approximately equal size subsets. Then, iteratively, one of these subsets (folds) is designated as a validation set and the remaining $k-1$ subsets(folds) are used to fit the model. Once the model is fit, predictions are made using the designated validation set and the mean square error $MSE_i\\approx \\lfloor\\frac{k}{n}\\rfloor\\sum_{j=1}^{\\lfloor\\frac{n}{k}\\rfloor}(y_{ij}-\\hat{y_{ij}})^2$ is computed for $i=1,2,\\dots,k$. Finally, the $k-$fold CV estimate of the test $MSE$ is the average of these values:\n",
    "\n",
    "$$CV_{(k)}=\\frac{1}{k}\\sum_{i=1}^kMSE_i$$\n",
    "\n",
    "As we mentioned, when $k=1$ then $k-$fold CV is simply LOOCV.\n",
    "\n",
    "In contract to LOOCV the $k-$fold CV is computationaly less expensive because instead of fitting the model $n$ times, it only requires fitting it $k$ times, where $k$ is typically taken to be $3,5$ or $10$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
