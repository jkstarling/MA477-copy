{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ====================================================</h2>\n",
    " <h1>MA477 - Theory and Applications of Data Science</h1> \n",
    "  <h1>Lesson 25: Neural Networks (Part 2)</h1> \n",
    " \n",
    " <h4>Dr. Valmir Bucaj</h4>\n",
    " <br>\n",
    " United States Military Academy, West Point, AY20-2\n",
    "<h2>=====================================================</h2>\n",
    "\n",
    "<h2>Lecture Outline</h2>\n",
    "\n",
    "<ul>\n",
    "    <li> Review of the LogReg Algorithm(viewed as a Perceptron)</li>\n",
    "    <li>Importance of Vectorization?</li>\n",
    "    <li> Vectorization of LogReg Algorithm\n",
    "    <ol>\n",
    "        <li> Forward Propagation</li>\n",
    "        <li> Backward Propagation</li>\n",
    "        <li>Vectorized Algorithm</li>\n",
    "        </ol></li>\n",
    "    <li>Python Implementation</li>\n",
    "   \n",
    "        \n",
    "        \n",
    "   </li>\n",
    "    \n",
    " </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Algorithm</h2>\n",
    "\n",
    "For this specific example, one iteration/step of the gradient descent the algorithm would be as follows:\n",
    "\n",
    "Initiate $w=(w_1,w_2), b$ and $J=0,dw=0,db=0$\n",
    "\n",
    "\n",
    "For $i$ in $range(1,m)$:\n",
    "\n",
    "$\\hspace{1cm}z^{(i)}=w^Tx^{(i)}+b^{(i)}$\n",
    "\n",
    "$\\hspace{1cm}a^{(i)}=\\sigma(z^{(i)})$\n",
    "\n",
    "$\\hspace{1cm}J+=-\\left[y^{(i)}\\log\\left(a^{(i)}\\right)+\\left(1-y^{(i)}\\right)\\log\\left(1-a^{(i)}\\right)\\right]$\n",
    "\n",
    "$\\hspace{1cm}$--- This is where Forward Propagation ends and Back Propagation begins! ---\n",
    "\n",
    "$\\hspace{1cm}dz^{(i)}=a^{(i)}-y^{(i)}$\n",
    "\n",
    "$\\hspace{1cm}db+=dz^{(i)}$\n",
    "\n",
    "\n",
    "$\\hspace{1cm}$ For j in range(1,$n_x=2$): \n",
    "\n",
    "$\\hspace{2cm}dw_j+=x_j^{(i)}\\,dz^{(i)}$\n",
    "\n",
    "\n",
    "\n",
    "$J:=J/m,\\, dw_j:=dw_j/m,\\, db:=db/m$\n",
    "\n",
    "(Updating Weights:)\n",
    "\n",
    "$w_j:=w_j-\\alpha\\,dw_j$\n",
    "\n",
    "$b :=b-\\alpha\\, db$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importance of Vectorization</h2>\n",
    "\n",
    "The purpose of vectorization for us will be to get rid of the <b> for loops</b>. For loops slow down computations significantly, as we will demonstrate below with a simple example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.randn(10000000)\n",
    "y=np.random.randn(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Non-Vectorized Computation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "tot=0\n",
    "for i,j in zip(x,y):\n",
    "    tot+=i*j\n",
    "end=time.time()\n",
    "\n",
    "print('Non-Vectorized Product: {}; \\n Time: {}'.format(tot,1000*(end-start))+' in ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Vectorized Computation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "tot=np.dot(x,y)\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "t=end-start\n",
    "\n",
    "print('Vectorized Product: {} \\n Time: {}'.format(tot, 1000*t)+ ' in ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the non-vectorized version took over 300 times longer to run. This was a single for-loop in a very simple scenario. We can certainly imagine more complicated computations taking place inside for-loops (e.g. in a NN), which would slow down the computations significantly. That's why, whenever possible it is imperative that we vectorize the code. We shall do so for LogReg below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorizing LogReg </h2>\n",
    "\n",
    "The first thing we are going to do is get rid of the second loop in our algorithm; that is, the loop that individually updates all of the weight changes $dw_j$. We can vectorize this step by defining \n",
    "\n",
    "$$dw=\\begin{bmatrix}dw_1\\\\ \\vdots \\\\ dw_{n_x}\\end{bmatrix}$$\n",
    "\n",
    "Then, we begin by initializing the weight changes as follows:\n",
    "\n",
    "$$dw=\\begin{bmatrix}0\\\\ \\vdots\\\\0\\end{bmatrix}_{n_x\\times 1}$$\n",
    "\n",
    "Then we can get rid of the following <b>for-loop</b>: \n",
    "\n",
    "$\\hspace{1cm}$ For j in range(1,$n_x$): \n",
    "\n",
    "$\\hspace{2cm}dw_j+=x_j^{(i)}\\,dz^{(i)}$\n",
    "\n",
    "by simply rewriting it as $$dw+=x^{(i)}dz^{(i)}=\\begin{bmatrix}x_1^{(i)}dz^{(i)}\\\\ \\vdots \\\\x_m^{(i)}dz^{(i)}\\end{bmatrix}\\hspace{.4cm} \\text{ where } x^{(i)}=\\begin{bmatrix}x_1^{(i)}\\\\ \\vdots \\\\x_m^{(i)}\\end{bmatrix}$$\n",
    "\n",
    "So, for each training sample $i$ all of the weight changes get updated simultaneously.\n",
    "\n",
    "In what follows we get rid of the outside for-loop by vectorizing it as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward Propagation</h4>\n",
    "\n",
    "Suppose we have $m$ training samples $\\left\\{x^{(i)}\\right\\}_{i=1}^m$ where $x^{(i)}\\in \\mathbb{R}^{n_x}$. Let's repackage the training samples into an $n_x$ by $m$ matrix $X$, by inserting each training sample $x^{(i)}$ as a column of $X$:\n",
    "\n",
    "\n",
    "$$X=\\Big[x^{(1)}\\dots x^{(m)}\\Big]\\, \\text{ where } \\, x^{(i)}=\\begin{bmatrix}x_1^{(i)}\\\\ \\vdots \\\\ x_{n_x}^{(i)}\\end{bmatrix},\\, \\text{ so } X\\in \\mathbb{R}^{n_x\\times m}$$\n",
    "\n",
    "Similarly, let the weight and bias vector be $$ w=\\begin{bmatrix}w_1\\\\\\vdots\\\\ w_{n_x}\\end{bmatrix},\\hspace{.5cm} B=\\big[b^{(1)}\\dots b^{(m)}\\big]$$\n",
    "\n",
    "Then, we can compute the forward propogation of the NN as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "Z&=w^TX+B\\\\\n",
    "&=\\Big[w^Tx^{(1)}+b^{(1)}\\dots w^Tx^{(m)}+b^{(m)}\\Big]\\\\\n",
    "&=\\Big[z^{(1)}\\dots z^{(m)}\\Big]_{1\\times m}\n",
    "\\end{align*}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align*}\n",
    "A&=\\sigma(Z)\\\\\n",
    "&=\\Big[\\sigma\\left(z^{(1)}\\right)\\dots \\sigma\\left(z^{(m)}\\right)\\Big]\\\\\n",
    "&=\\Big[a^{(1)}\\dots a^{(m)}\\Big]_{1\\times m}\n",
    "\\end{align*}\n",
    "\n",
    "So, we are done with the forward propagation; that is, with computing the output of the NN without ever using a single for-loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Backward Propagation</h4>\n",
    "\n",
    "Recall, that in <b>backpropagation</b> the key piece was to compute $dz$ since it is the only additional info we need to compute $dw$ and $db$. So, in the vectorized version we may define:\n",
    "\n",
    "$$dZ=\\Big[dz^{(1)}\\dots dz^{(m)}\\Big]_{1\\times m}, \\hspace{.4cm} \\text{ where } dz^{(i)}=a^{(i)}-y^{(i)}.$$\n",
    "\n",
    "So, if we also define $$Y=\\big[y^{(1)}\\dots y^{(m)}\\big]_{1\\times m}$$ then we get \n",
    "\n",
    "$$dZ=A-Y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are finally in a position to compute $db$ and $dw$.\n",
    "\n",
    "$$db=\\frac{1}{m}\\sum_{i=1}^mdz^{(i)}\\Rightarrow \\fbox{$db=\\frac{1}{m}np.sum(dZ)$}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$dw=\\frac{1}{m}XdZ^T=\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)}& x_1^{(2)}&\\dots &x_1^{(m)}\\\\\n",
    "x_2^{(1)}&x_2^{(2)}&\\dots &x_2^{(m)}\\\\\n",
    "\\vdots &&&\\vdots\\\\\n",
    "x_{n_x}^{(1)}&x_{n_x}^{(2)}&\\dots &x_{n_x}^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "dz^{(1)}\\\\\n",
    "dz^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "dz^{(m)}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^{m}x_1^{(i)}dz^{(i)}\\\\ \\vdots\\\\ \\frac{1}{m}\\sum_{i=1}^{m}x_{n_x}^{(i)}dz^{(i)}\\end{bmatrix}_{n_x\\times 1}$$\n",
    "\n",
    "We are done! Now we can implement the vectorized version of the LogReg Algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Vectorized Algorithm</h4>\n",
    "\n",
    "We can complete one pass (forward and backward propagation) withoug using a single <b>for-loop</b>. However, if we want to perform more than one iteration (which in practice we always do), then we still need to have one loop. Below we descrbe the vectorized version of the LogReg Algorithm viewed as a NN (This is the same as a Perceptron!):\n",
    "\n",
    "For <b>iter</b> in range(1,N):\n",
    "\n",
    "$\\hspace{1cm}Z=w^TX+B$\n",
    "\n",
    "$\\hspace{1cm}A=\\sigma(Z)$\n",
    "\n",
    "$\\hspace{1cm}dZ=A-Y$\n",
    "\n",
    "$\\hspace{1cm}db=\\frac{1}{m}np.sum(dZ)$\n",
    "\n",
    "$\\hspace{1cm}dw=\\frac{1}{m}XdZ^T$\n",
    "\n",
    "$\\hspace{1cm}J+=-\\frac{1}{N}\\big[Y\\log(A^T)+(1-Y)\\log(1-A^T)\\big]$\n",
    "\n",
    "$\\hspace{1cm} w:=w-\\alpha dw$\n",
    "\n",
    "$\\hspace{1cm} b:=b-\\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Python Implementation</h2>\n",
    "\n",
    "<font size='5' color='red'>Exercise:</font> <font size=4>Implement the above algorithm (Perceptron) in Python. Once you've done so, then train the Perceptron to make predictions using the airline satisfaction data set. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_bias_init(X):\n",
    "    #W is a X.shape[0] by 1 matrix (each row of X represents a feature)\n",
    "    #b is a 1 by X.shape[0]\n",
    "    \n",
    "    W=np.random.normal(scale=0.01,size=(X.shape[0],1))\n",
    "    b=0\n",
    "    #np.zeros(shape=(1,X.shape[1]))\n",
    "    #print(W.T.shape)\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,W,b):\n",
    "\n",
    "    y_out=np.matmul(W.T,X)+b\n",
    "    \n",
    "    y_out=sigmoid(y_out)\n",
    "    #print(y_out[0])\n",
    "    return y_out[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_funct(X,y_in,y_out):\n",
    "    m=X.shape[1]\n",
    "    return -(1/m)*(np.matmul(y_in,np.log(y_out.T))+np.matmul(1-y_in,np.log(1-y_out.T)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update(X,W,b,y_in,y_out,learning_rate):\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    dZ=y_out-y_in\n",
    "    dw=(1/m)*np.matmul(X,dZ.T)\n",
    "    db=(1/m)*np.sum(dZ)\n",
    "    \n",
    "    W=W-learning_rate*dw\n",
    "    b=b-db\n",
    "    \n",
    "    return W,b\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,y,n_iter,learning_rate):\n",
    "    \"\"\"\n",
    "    X=input data\n",
    "    y=response variable\n",
    "    n_iter=number of iterations to run\n",
    "    learning_rate=rate at which to update the weights\n",
    "    \n",
    "    \"\"\"\n",
    "    #weight and bias initialization\n",
    "    W,b=weight_bias_init(X)\n",
    "    \n",
    "    #forward pass\n",
    "    \n",
    "    for i in range(1,n_iter):\n",
    "        \n",
    "        cost=0\n",
    "        \n",
    "        y_out=forward_propagation(X,W,b)\n",
    "        \n",
    "        cost+=cost_funct(X,y_in=y,y_out=y_out)/n_iter\n",
    "        print('Iteration {} ... Cost: {:.4f}'.format(i,cost))\n",
    "        \n",
    "        W,b=weight_update(X,W,b,y,y_out,learning_rate)\n",
    "        \n",
    "    print('Total Cost {}'.format(cost))\n",
    "    \n",
    "    return W,b\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X,W,b):\n",
    "    y_pred=[]\n",
    "    y_out=forward_propagation(X,W,b)\n",
    "    #print(y_out)\n",
    "    \n",
    "    for y in y_out:\n",
    "        if y>0.5:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    return y_out, y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_marks(n_cols, batch_size):\n",
    "    return range(batch_size, math.ceil(n_cols / batch_size) * batch_size, batch_size)\n",
    "\n",
    "def split_dataframe(data_frame, batch_size):\n",
    "    indices = index_marks(data_frame.shape[1], batch_size)\n",
    "    data=np.split(data_frame, indices,axis=1)\n",
    "#     X=data.drop(data.columns[-1],axis=1)\n",
    "#     y=data[data.columns[-1]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is Optional!\n",
    "\n",
    "def train_model_batch(X,y,learning_rate,epochs,batch_size):\n",
    "    \"\"\"\n",
    "    X=input data\n",
    "    y=response variable\n",
    "    n_iter=number of iterations to run\n",
    "    learning_rate=rate at which to update the weights\n",
    "    \n",
    "    \"\"\"\n",
    "    #weight and bias initialization\n",
    "    W,b=weight_bias_init(X)\n",
    "    \n",
    "    X.loc['target',X.columns]=y\n",
    "#     print(X.head())\n",
    "   \n",
    "\n",
    "    total_batches=int(X.shape[1]/batch_size)\n",
    "    \n",
    "    #forward pass\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        X_lst=split_dataframe(X,batch_size)\n",
    "        cost=0\n",
    "        \n",
    "        for i in range(total_batches):\n",
    "            y_new=X_lst[i].loc[X_lst[i].index[-1]]\n",
    "            X_new=X_lst[i].drop(X_lst[i].index[-1])\n",
    "            \n",
    "            X_new=np.array(X_new)\n",
    "            y_new=np.array(y_new)\n",
    "            \n",
    "               \n",
    "            y_out=forward_propagation(X_new,W,b)\n",
    "\n",
    "            W,b=weight_update(X_new,W,b,y_new,y_out,learning_rate)\n",
    "        \n",
    "        y_o=forward_propagation(np.array(X.drop(X.index[-1])),W,b)\n",
    "       \n",
    "        cost+=cost_funct(X,y_in=np.array(y),y_out=y_o)/total_batches\n",
    "        print('Epoch {} ... Cost: {:.4f}'.format(epoch,cost))\n",
    "        \n",
    "    print('Total Cost {}'.format(cost))\n",
    "    \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We import the airline satisfaction data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline=pd.read_csv('airline_train.csv',index_col=[0])\n",
    "airline.drop(airline.columns[:2],inplace=True,axis=1)\n",
    "airline.dropna(inplace=True)\n",
    "airline.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "\n",
    "rows=random.sample(list(airline.index),35000)\n",
    "airline=airline.loc[rows]\n",
    "\n",
    "airline.reset_index(inplace=True,drop=True)\n",
    "airline['satisfaction']=airline['satisfaction'].apply(lambda x: 1 if x=='satisfied' else 0)\n",
    "X=airline.drop('satisfaction',axis=1)\n",
    "y=airline['satisfaction']\n",
    "\n",
    "\n",
    "#Convert qualitative variables to numerical dummy variables\n",
    "\n",
    "cols=['Gender','Customer Type', 'Type of Travel','Class']\n",
    "\n",
    "X=pd.get_dummies(data=X,columns=cols,drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "\n",
    "scaled_train=scaler.fit_transform(X_train)\n",
    "X_train_sc=pd.DataFrame(scaled_train,columns=X_train.columns,index=X_train.index)\n",
    "\n",
    "\n",
    "scaled_val=scaler.fit_transform(X_val)\n",
    "X_val_sc=pd.DataFrame(scaled_val,columns=X_val.columns,index=X_val.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we are using NumPy under the hood, we need to convert dataframes to numpy arrays first\n",
    "\n",
    "X_t=np.array(X_train_sc.T)\n",
    "y_t=np.array(y_train)\n",
    "X_v=np.array(X_val_sc.T)\n",
    "y_v=np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "\n",
    "#W,b=train_model(X_t,y_t,learning_rate=0.015,n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob,y_pred=model_predict(X_v,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_v,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Batch Training</h4>\n",
    "\n",
    "Typically, instead of feeding in the entire data set into the gradient descent algorithm, you do it in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W,b=train_model_batch(X_train_sc.T,y_train,learning_rate=0.01,epochs=15,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v=np.array(X_val_sc.T)\n",
    "y_v=np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out,y_pred=model_predict(X_v,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_v,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
