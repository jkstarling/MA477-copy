{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ====================================================</h2>\n",
    " <h1>MA477 - Theory and Applications of Data Science</h1> \n",
    "  <h1>Lesson 24: Neural Networks (Part 2)</h1> \n",
    " \n",
    " <h4>Dr. Valmir Bucaj</h4>\n",
    " <br>\n",
    " United States Military Academy, West Point, AY20-2\n",
    "<h2>=====================================================</h2>\n",
    "\n",
    "<h2>Lecture Outline</h2>\n",
    "\n",
    "<ul>\n",
    "    <li> Review of the LogReg Algorithm(viewed as a Perceptron)</li>\n",
    "    <li>Importance of Vectorization?</li>\n",
    "    <li> Vectorization of LogReg Algorithm\n",
    "    <ol>\n",
    "        <li> Forward Propagation</li>\n",
    "        <li> Backward Propagation</li>\n",
    "        <li>Vectorized Algorithm</li>\n",
    "        </ol></li>\n",
    "    <li>Python Implementation</li>\n",
    "   \n",
    "        \n",
    "        \n",
    "   </li>\n",
    "    \n",
    " </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Algorithm</h2>\n",
    "\n",
    "For this specific example, one iteration/step of the gradient descent the algorithm would be as follows:\n",
    "\n",
    "Initiate $w=(w_1,w_2), b$ and $J=0,dw=0,db=0$\n",
    "\n",
    "\n",
    "For $i$ in $range(1,m)$:\n",
    "\n",
    "$\\hspace{1cm}z^{(i)}=w^Tx^{(i)}+b^{(i)}$\n",
    "\n",
    "$\\hspace{1cm}a^{(i)}=\\sigma(z^{(i)})$\n",
    "\n",
    "$\\hspace{1cm}J+=-\\left[y^{(i)}\\log\\left(a^{(i)}\\right)+\\left(1-y^{(i)}\\right)\\log\\left(1-a^{(i)}\\right)\\right]$\n",
    "\n",
    "$\\hspace{1cm}$--- This is where Forward Propagation ends and Back Propagation begins! ---\n",
    "\n",
    "$\\hspace{1cm}dz^{(i)}=a^{(i)}-y^{(i)}$\n",
    "\n",
    "$\\hspace{1cm}db+=dz^{(i)}$\n",
    "\n",
    "\n",
    "$\\hspace{1cm}$ For j in range(1,$n_x=2$): \n",
    "\n",
    "$\\hspace{2cm}dw_j+=x_j^{(i)}\\,dz^{(i)}$\n",
    "\n",
    "\n",
    "\n",
    "$J:=J/m,\\, dw_j:=dw_j/m,\\, db:=db/m$\n",
    "\n",
    "(Updating Weights:)\n",
    "\n",
    "$w_j:=w_j-\\alpha\\,dw_j$\n",
    "\n",
    "$b :=b-\\alpha\\, db$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importance of Vectorization</h2>\n",
    "\n",
    "The purpose of vectorization for us will be to get rid of the <b> for loops</b>. For loops slow down computations significantly, as we will demonstrate below with a simple example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.randn(10000000)\n",
    "y=np.random.randn(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Non-Vectorized Computation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Vectorized Product: 1865.2496132198976; \n",
      " Time: 2779.8073291778564 in ms\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "tot=0\n",
    "for i,j in zip(x,y):\n",
    "    tot+=i*j\n",
    "end=time.time()\n",
    "\n",
    "print('Non-Vectorized Product: {}; \\n Time: {}'.format(tot,1000*(end-start))+' in ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Vectorized Computation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Product: 1865.2496132195715 \n",
      " Time: 9.82975959777832 in ms\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "tot=np.dot(x,y)\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "t=end-start\n",
    "\n",
    "print('Vectorized Product: {} \\n Time: {}'.format(tot, 1000*t)+ ' in ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the non-vectorized version took over 300 times longer to run. This was a single for-loop in a very simple scenario. We can certainly imagine more complicated computations taking place inside for-loops (e.g. in a NN), which would slow down the computations significantly. That's why, whenever possible it is imperative that we vectorize the code. We shall do so for LogReg below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorizing LogReg </h2>\n",
    "\n",
    "The first thing we are going to do is get rid of the second loop in our algorithm; that is, the loop that individually updates all of the weight changes $dw_j$. We can vectorize this step by defining \n",
    "\n",
    "$$dw=\\begin{bmatrix}dw_1\\\\ \\vdots \\\\ dw_{n_x}\\end{bmatrix}$$\n",
    "\n",
    "Then, we begin by initializing the weight changes as follows:\n",
    "\n",
    "$$dw=\\begin{bmatrix}0\\\\ \\vdots\\\\0\\end{bmatrix}_{n_x\\times 1}$$\n",
    "\n",
    "Then we can get rid of the following <b>for-loop</b>: \n",
    "\n",
    "$\\hspace{1cm}$ For j in range(1,$n_x$): \n",
    "\n",
    "$\\hspace{2cm}dw_j+=x_j^{(i)}\\,dz^{(i)}$\n",
    "\n",
    "by simply rewriting it as $$dw+=x^{(i)}dz^{(i)}=\\begin{bmatrix}x_1^{(i)}dz^{(i)}\\\\ \\vdots \\\\x_m^{(i)}dz^{(i)}\\end{bmatrix}\\hspace{.4cm} \\text{ where } x^{(i)}=\\begin{bmatrix}x_1^{(i)}\\\\ \\vdots \\\\x_m^{(i)}\\end{bmatrix}$$\n",
    "\n",
    "So, for each training sample $i$ all of the weight changes get updated simultaneously.\n",
    "\n",
    "In what follows we get rid of the outside for-loop by vectorizing it as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forward Propagation</h4>\n",
    "\n",
    "Suppose we have $m$ training samples $\\left\\{x^{(i)}\\right\\}_{i=1}^m$ where $x^{(i)}\\in \\mathbb{R}^{n_x}$. Let's repackage the training samples into an $n_x$ by $m$ matrix $X$, by inserting each training sample $x^{(i)}$ as a column of $X$:\n",
    "\n",
    "\n",
    "$$X=\\Big[x^{(1)}\\dots x^{(m)}\\Big]\\, \\text{ where } \\, x^{(i)}=\\begin{bmatrix}x_1^{(i)}\\\\ \\vdots \\\\ x_{n_x}^{(i)}\\end{bmatrix},\\, \\text{ so } X\\in \\mathbb{R}^{n_x\\times m}$$\n",
    "\n",
    "Similarly, let the weight and bias vector be $$ w=\\begin{bmatrix}w_1\\\\\\vdots\\\\ w_{n_x}\\end{bmatrix},\\hspace{.5cm} B=\\big[b^{(1)}\\dots b^{(m)}\\big]$$\n",
    "\n",
    "Then, we can compute the forward propogation of the NN as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "Z&=w^TX+B\\\\\n",
    "&=\\Big[w^Tx^{(1)}+b^{(1)}\\dots w^Tx^{(m)}+b^{(m)}\\Big]\\\\\n",
    "&=\\Big[z^{(1)}\\dots z^{(m)}\\Big]_{1\\times m}\n",
    "\\end{align*}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align*}\n",
    "A&=\\sigma(Z)\\\\\n",
    "&=\\Big[\\sigma\\left(z^{(1)}\\right)\\dots \\sigma\\left(z^{(m)}\\right)\\Big]\\\\\n",
    "&=\\Big[a^{(1)}\\dots a^{(m)}\\Big]_{1\\times m}\n",
    "\\end{align*}\n",
    "\n",
    "So, we are done with the forward propagation; that is, with computing the output of the NN without ever using a single for-loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Backward Propagation</h4>\n",
    "\n",
    "Recall, that in <b>backpropagation</b> the key piece was to compute $dz$ since it is the only additional info we need to compute $dw$ and $db$. So, in the vectorized version we may define:\n",
    "\n",
    "$$dZ=\\Big[dz^{(1)}\\dots dz^{(m)}\\Big]_{1\\times m}, \\hspace{.4cm} \\text{ where } dz^{(i)}=a^{(i)}-y^{(i)}.$$\n",
    "\n",
    "So, if we also define $$Y=\\big[y^{(1)}\\dots y^{(m)}\\big]_{1\\times m}$$ then we get \n",
    "\n",
    "$$dZ=A-Y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are finally in a position to compute $db$ and $dw$.\n",
    "\n",
    "$$db=\\frac{1}{m}\\sum_{i=1}^mdz^{(i)}\\Rightarrow \\fbox{$db=\\frac{1}{m}np.sum(dZ)$}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$dw=\\frac{1}{m}XdZ^T=\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)}& x_1^{(2)}&\\dots &x_1^{(m)}\\\\\n",
    "x_2^{(1)}&x_2^{(2)}&\\dots &x_2^{(m)}\\\\\n",
    "\\vdots &&&\\vdots\\\\\n",
    "x_{n_x}^{(1)}&x_{n_x}^{(2)}&\\dots &x_{n_x}^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "dz^{(1)}\\\\\n",
    "dz^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "dz^{(m)}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^{m}x_1^{(i)}dz^{(i)}\\\\ \\vdots\\\\ \\frac{1}{m}\\sum_{i=1}^{m}x_{n_x}^{(i)}dz^{(i)}\\end{bmatrix}_{n_x\\times 1}$$\n",
    "\n",
    "We are done! Now we can implement the vectorized version of the LogReg Algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Vectorized Algorithm</h4>\n",
    "\n",
    "We can complete one pass (forward and backward propagation) withoug using a single <b>for-loop</b>. However, if we want to perform more than one iteration (which in practice we always do), then we still need to have one loop. Below we descrbe the vectorized version of the LogReg Algorithm viewed as a NN (This is the same as a Perceptron!):\n",
    "\n",
    "For <b>iter</b> in range(1,N):\n",
    "\n",
    "$\\hspace{1cm}Z=w^TX+B$\n",
    "\n",
    "$\\hspace{1cm}A=\\sigma(Z)$\n",
    "\n",
    "$\\hspace{1cm}dZ=A-Y$\n",
    "\n",
    "$\\hspace{1cm}db=\\frac{1}{m}np.sum(dZ)$\n",
    "\n",
    "$\\hspace{1cm}dw=\\frac{1}{m}XdZ^T$\n",
    "\n",
    "$\\hspace{1cm}J+=-\\frac{1}{N}\\big[Y\\log(A^T)+(1-Y)\\log(1-A^T)\\big]$\n",
    "\n",
    "$\\hspace{1cm} w:=w-\\alpha dw$\n",
    "\n",
    "$\\hspace{1cm} b:=b-\\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Python Implementation</h2>\n",
    "\n",
    "<font size='5' color='red'>Exercise:</font> <font size=4>Implement the above algorithm (Perceptron) in Python. Once you've done so, then train the Perceptron to make predictions using the airline satisfaction data set. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
