{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ======================================================</h2>\n",
    " <h1>MA477 - Theory and Applications of Data Science</h1> \n",
    "  <h1>Lesson 15: Tokenization, Speech Tagging, Chunking</h1> \n",
    " \n",
    " <h4>Dr. Valmir Bucaj</h4>\n",
    " United States Military Academy, West Point \n",
    "AY20-2\n",
    "<h2>======================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lecture Outline</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>Tokenization</li>\n",
    "    <li> Normalizing</li>\n",
    "    <li>Tagging Part of Speech</li>\n",
    "    <li>Chunking</li>\n",
    "    \n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>\n",
    "\n",
    "Tokenization is the process of breaking a text down into words or sentences. When dealing with text, typically they don't come already broken down into words or sentences, so it's up to us to do so. NLTK has a built-in method that easily breaks text down into words or sentences. \n",
    "\n",
    "Below we'll use some text describing the coronavirus as our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"What are Coronaviruses? Coronaviruses (CoV) are a large family of viruses that cause illness \n",
    "ranging from the common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS-CoV) and \n",
    "Severe Acute Respiratory Syndrome (SARS-CoV). A novel coronavirus (nCoV) is a new strain that has not \n",
    "been previously identified in humans! \n",
    "Coronaviruses are zoonotic; meaning they are transmitted between animals and people.  \n",
    "\n",
    "Detailed investigations found that SARS-CoV was transmitted from civet cats to humans and MERS-CoV from \n",
    "dromedary camels to humans. Several known coronaviruses are circulating in animals that have not yet infected \n",
    "humans. \n",
    "\n",
    "Common signs of infection include respiratory symptoms, fever, cough, shortness of breath and breathing\n",
    "difficulties. In more severe cases, infection can cause pneumonia, severe acute respiratory syndrome, kidney \n",
    "failure and even death. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll tokenize the text into words. In other words, we will break-down the text at each whitespace and punctuation signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens=nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'are',\n",
       " 'Coronaviruses',\n",
       " '?',\n",
       " 'Coronaviruses',\n",
       " '(',\n",
       " 'CoV',\n",
       " ')',\n",
       " 'are',\n",
       " 'a',\n",
       " 'large',\n",
       " 'family',\n",
       " 'of',\n",
       " 'viruses',\n",
       " 'that']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tokenize by sentences. That is, we can break-down text at every punctuation mark that indicates the end of a sentence(e.g a period, question mark, exclamation mark etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_tokens=nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are Coronaviruses?',\n",
       " 'Coronaviruses (CoV) are a large family of viruses that cause illness \\nranging from the common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS-CoV) and \\nSevere Acute Respiratory Syndrome (SARS-CoV).',\n",
       " 'A novel coronavirus (nCoV) is a new strain that has not \\nbeen previously identified in humans!',\n",
       " 'Coronaviruses are zoonotic; meaning they are transmitted between animals and people.',\n",
       " 'Detailed investigations found that SARS-CoV was transmitted from civet cats to humans and MERS-CoV from \\ndromedary camels to humans.',\n",
       " 'Several known coronaviruses are circulating in animals that have not yet infected \\nhumans.',\n",
       " 'Common signs of infection include respiratory symptoms, fever, cough, shortness of breath and breathing\\ndifficulties.',\n",
       " 'In more severe cases, infection can cause pneumonia, severe acute respiratory syndrome, kidney \\nfailure and even death.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to tokenize into words each of the sentences then one way to do so is via a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[nltk.word_tokenize(sent) for sent in sents_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['What', 'are', 'Coronaviruses', '?'],\n",
       " ['Coronaviruses',\n",
       "  '(',\n",
       "  'CoV',\n",
       "  ')',\n",
       "  'are',\n",
       "  'a',\n",
       "  'large',\n",
       "  'family',\n",
       "  'of',\n",
       "  'viruses',\n",
       "  'that',\n",
       "  'cause',\n",
       "  'illness',\n",
       "  'ranging',\n",
       "  'from',\n",
       "  'the',\n",
       "  'common',\n",
       "  'cold',\n",
       "  'to',\n",
       "  'more',\n",
       "  'severe',\n",
       "  'diseases',\n",
       "  'such',\n",
       "  'as',\n",
       "  'Middle',\n",
       "  'East',\n",
       "  'Respiratory',\n",
       "  'Syndrome',\n",
       "  '(',\n",
       "  'MERS-CoV',\n",
       "  ')',\n",
       "  'and',\n",
       "  'Severe',\n",
       "  'Acute',\n",
       "  'Respiratory',\n",
       "  'Syndrome',\n",
       "  '(',\n",
       "  'SARS-CoV',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['A',\n",
       "  'novel',\n",
       "  'coronavirus',\n",
       "  '(',\n",
       "  'nCoV',\n",
       "  ')',\n",
       "  'is',\n",
       "  'a',\n",
       "  'new',\n",
       "  'strain',\n",
       "  'that',\n",
       "  'has',\n",
       "  'not',\n",
       "  'been',\n",
       "  'previously',\n",
       "  'identified',\n",
       "  'in',\n",
       "  'humans',\n",
       "  '!']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Normalization</h2>\n",
    "\n",
    "When performing text analysis often we want only to look at the words and get rid of all the punctuation and other meaningless characters. The process of extracting only the words out of a text is typically knonw as text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_1=tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronaviruses',\n",
       " '(',\n",
       " 'CoV',\n",
       " ')',\n",
       " 'are',\n",
       " 'a',\n",
       " 'large',\n",
       " 'family',\n",
       " 'of',\n",
       " 'viruses',\n",
       " 'that',\n",
       " 'cause',\n",
       " 'illness',\n",
       " 'ranging',\n",
       " 'from',\n",
       " 'the',\n",
       " 'common',\n",
       " 'cold',\n",
       " 'to',\n",
       " 'more',\n",
       " 'severe',\n",
       " 'diseases',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'Respiratory',\n",
       " 'Syndrome',\n",
       " '(',\n",
       " 'MERS-CoV',\n",
       " ')',\n",
       " 'and',\n",
       " 'Severe',\n",
       " 'Acute',\n",
       " 'Respiratory',\n",
       " 'Syndrome',\n",
       " '(',\n",
       " 'SARS-CoV',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronaviruses\n",
      "CoV\n",
      "are\n",
      "a\n",
      "large\n",
      "family\n",
      "of\n",
      "viruses\n",
      "that\n",
      "cause\n",
      "illness\n",
      "ranging\n",
      "from\n",
      "the\n",
      "common\n",
      "cold\n",
      "to\n",
      "more\n",
      "severe\n",
      "diseases\n",
      "such\n",
      "as\n",
      "Middle\n",
      "East\n",
      "Respiratory\n",
      "Syndrome\n",
      "and\n",
      "Severe\n",
      "Acute\n",
      "Respiratory\n",
      "Syndrome\n"
     ]
    }
   ],
   "source": [
    "for word in covid_1:\n",
    "    if word.isalpha():\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the entire `text`. You can either use a `for` loop or use list `comprehension`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_norm=[[word for word in item if word.isalpha()] for item in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronaviruses',\n",
       " 'CoV',\n",
       " 'are',\n",
       " 'a',\n",
       " 'large',\n",
       " 'family',\n",
       " 'of',\n",
       " 'viruses',\n",
       " 'that',\n",
       " 'cause',\n",
       " 'illness',\n",
       " 'ranging',\n",
       " 'from',\n",
       " 'the',\n",
       " 'common',\n",
       " 'cold',\n",
       " 'to',\n",
       " 'more',\n",
       " 'severe',\n",
       " 'diseases',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'Respiratory',\n",
       " 'Syndrome',\n",
       " 'and',\n",
       " 'Severe',\n",
       " 'Acute',\n",
       " 'Respiratory',\n",
       " 'Syndrome']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_norm[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times when doing text analysis, such as frequency distribution, we don't want to distinguish between Data, data, DATA, dAta, etc. In other words, we don't want them to count as different tokens but rather as the same token. To avoid things like this, we often may want to convert the entire text to either lower or upper case.\n",
    "\n",
    "For example, let's turn everything in our text tokens into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lower=[[word.lower() for word in item if word.isalpha()] for item in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'are', 'coronaviruses'],\n",
       " ['coronaviruses',\n",
       "  'cov',\n",
       "  'are',\n",
       "  'a',\n",
       "  'large',\n",
       "  'family',\n",
       "  'of',\n",
       "  'viruses',\n",
       "  'that',\n",
       "  'cause',\n",
       "  'illness',\n",
       "  'ranging',\n",
       "  'from',\n",
       "  'the',\n",
       "  'common',\n",
       "  'cold',\n",
       "  'to',\n",
       "  'more',\n",
       "  'severe',\n",
       "  'diseases',\n",
       "  'such',\n",
       "  'as',\n",
       "  'middle',\n",
       "  'east',\n",
       "  'respiratory',\n",
       "  'syndrome',\n",
       "  'and',\n",
       "  'severe',\n",
       "  'acute',\n",
       "  'respiratory',\n",
       "  'syndrome'],\n",
       " ['a',\n",
       "  'novel',\n",
       "  'coronavirus',\n",
       "  'ncov',\n",
       "  'is',\n",
       "  'a',\n",
       "  'new',\n",
       "  'strain',\n",
       "  'that',\n",
       "  'has',\n",
       "  'not',\n",
       "  'been',\n",
       "  'previously',\n",
       "  'identified',\n",
       "  'in',\n",
       "  'humans'],\n",
       " ['coronaviruses',\n",
       "  'are',\n",
       "  'zoonotic',\n",
       "  'meaning',\n",
       "  'they',\n",
       "  'are',\n",
       "  'transmitted',\n",
       "  'between',\n",
       "  'animals',\n",
       "  'and',\n",
       "  'people'],\n",
       " ['detailed',\n",
       "  'investigations',\n",
       "  'found',\n",
       "  'that',\n",
       "  'was',\n",
       "  'transmitted',\n",
       "  'from',\n",
       "  'civet',\n",
       "  'cats',\n",
       "  'to',\n",
       "  'humans',\n",
       "  'and',\n",
       "  'from',\n",
       "  'dromedary',\n",
       "  'camels',\n",
       "  'to',\n",
       "  'humans'],\n",
       " ['several',\n",
       "  'known',\n",
       "  'coronaviruses',\n",
       "  'are',\n",
       "  'circulating',\n",
       "  'in',\n",
       "  'animals',\n",
       "  'that',\n",
       "  'have',\n",
       "  'not',\n",
       "  'yet',\n",
       "  'infected',\n",
       "  'humans'],\n",
       " ['common',\n",
       "  'signs',\n",
       "  'of',\n",
       "  'infection',\n",
       "  'include',\n",
       "  'respiratory',\n",
       "  'symptoms',\n",
       "  'fever',\n",
       "  'cough',\n",
       "  'shortness',\n",
       "  'of',\n",
       "  'breath',\n",
       "  'and',\n",
       "  'breathing',\n",
       "  'difficulties'],\n",
       " ['in',\n",
       "  'more',\n",
       "  'severe',\n",
       "  'cases',\n",
       "  'infection',\n",
       "  'can',\n",
       "  'cause',\n",
       "  'pneumonia',\n",
       "  'severe',\n",
       "  'acute',\n",
       "  'respiratory',\n",
       "  'syndrome',\n",
       "  'kidney',\n",
       "  'failure',\n",
       "  'and',\n",
       "  'even',\n",
       "  'death']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we don't want to distinguish between words such as dog and dogs, or woman and women, lie and lying or liar etc. In other words, maybe we only care if the words have the same stem so to speak. In this case, we may try to first use some normalization technique that corrects for the different references to essentially the same word.\n",
    "\n",
    "This may be accomplished via what's known as `Stemmers`. As we will shortly see though, they are imperfect and not always yield great results, so we have to be cautious when using stemmers. \n",
    "\n",
    "Let's begin by creating a list first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['cat','cats','dog','dogs','doggies','woman','women','lie','liar','lying','week','weekly','break','breaking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1_porter=[porter.stem(word) for word in list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'dog',\n",
       " 'doggi',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'liar',\n",
       " 'lie',\n",
       " 'week',\n",
       " 'weekli',\n",
       " 'break',\n",
       " 'break']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1_porter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO, the Porter Stemmer did fairly well, however it missed some. Let's see if another stemmer can do better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan=nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1_lan=[lan.stem(word) for word in list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'dog',\n",
       " 'doggy',\n",
       " 'wom',\n",
       " 'wom',\n",
       " 'lie',\n",
       " 'liar',\n",
       " 'lying',\n",
       " 'week',\n",
       " 'week',\n",
       " 'break',\n",
       " 'break']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1_lan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt to increase the accuracy of the stemmers, we can try to use them sequentially, one after the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'dog',\n",
       " 'dogg',\n",
       " 'wom',\n",
       " 'wom',\n",
       " 'lie',\n",
       " 'liar',\n",
       " 'lie',\n",
       " 'week',\n",
       " 'weekl',\n",
       " 'break',\n",
       " 'break']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lan.stem(word) for word in list1_porter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'dog',\n",
       " 'doggi',\n",
       " 'wom',\n",
       " 'wom',\n",
       " 'lie',\n",
       " 'liar',\n",
       " 'lie',\n",
       " 'week',\n",
       " 'week',\n",
       " 'break',\n",
       " 'break']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(word) for word in list1_lan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tagging Parts of Speech</h2>\n",
    "\n",
    "Often times when performing text analysis, and especially text summary etc., it may become very important knowing the parts of speech. So, being able to correctly identify the parts of speech and tag them accordingly may be a crucial step in providing a sophisticated solution to a problem concerning a large amount of text. For example maybe we want to extract only nouns from a text, or we want to count how many adjectives per sentence are there etc....tagging the text with the appropriate speech tag allowes us to do all of these and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=text_norm[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronaviruses',\n",
       " 'are',\n",
       " 'zoonotic',\n",
       " 'meaning',\n",
       " 'they',\n",
       " 'are',\n",
       " 'transmitted',\n",
       " 'between',\n",
       " 'animals',\n",
       " 'and',\n",
       " 'people']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\valmir.bucaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\valmir.bucaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2_tag=nltk.pos_tag(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Coronaviruses', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('zoonotic', 'JJ'),\n",
       " ('meaning', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('transmitted', 'VBN'),\n",
       " ('between', 'IN'),\n",
       " ('animals', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('people', 'NNS')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what each of these abbreviations mean we can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=4>Exercise</font>\n",
    "\n",
    "Find the most common nouns in the text above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are Coronaviruses? Coronaviruses (CoV) are a large family of viruses that cause illness \n",
      "ranging from the common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS-CoV) and \n",
      "Severe Acute Respiratory Syndrome (SARS-CoV). A novel coronavirus (nCoV) is a new strain that has not \n",
      "been previously identified in humans! \n",
      "Coronaviruses are zoonotic; meaning they are transmitted between animals and people.  \n",
      "\n",
      "Detailed investigations found that SARS-CoV was transmitted from civet cats to humans and MERS-CoV from \n",
      "dromedary camels to humans. Several known coronaviruses are circulating in animals that have not yet infected \n",
      "humans. \n",
      "\n",
      "Common signs of infection include respiratory symptoms, fever, cough, shortness of breath and breathing\n",
      "difficulties. In more severe cases, infection can cause pneumonia, severe acute respiratory syndrome, kidney \n",
      "failure and even death. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\valmir.bucaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tag=[word for word in nltk.pos_tag(nltk.word_tokenize(text), tagset='universal') if word[0].isalpha() and\n",
    "         word[1]=='NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Coronaviruses', 'NOUN'),\n",
       " ('Coronaviruses', 'NOUN'),\n",
       " ('CoV', 'NOUN'),\n",
       " ('family', 'NOUN'),\n",
       " ('viruses', 'NOUN'),\n",
       " ('cold', 'NOUN'),\n",
       " ('diseases', 'NOUN'),\n",
       " ('Middle', 'NOUN'),\n",
       " ('East', 'NOUN'),\n",
       " ('Respiratory', 'NOUN'),\n",
       " ('Syndrome', 'NOUN'),\n",
       " ('Severe', 'NOUN'),\n",
       " ('Acute', 'NOUN'),\n",
       " ('Respiratory', 'NOUN'),\n",
       " ('Syndrome', 'NOUN'),\n",
       " ('coronavirus', 'NOUN'),\n",
       " ('strain', 'NOUN'),\n",
       " ('humans', 'NOUN'),\n",
       " ('Coronaviruses', 'NOUN'),\n",
       " ('animals', 'NOUN'),\n",
       " ('people', 'NOUN'),\n",
       " ('investigations', 'NOUN'),\n",
       " ('civet', 'NOUN'),\n",
       " ('cats', 'NOUN'),\n",
       " ('humans', 'NOUN'),\n",
       " ('camels', 'NOUN'),\n",
       " ('humans', 'NOUN'),\n",
       " ('coronaviruses', 'NOUN'),\n",
       " ('animals', 'NOUN'),\n",
       " ('humans', 'NOUN'),\n",
       " ('Common', 'NOUN'),\n",
       " ('signs', 'NOUN'),\n",
       " ('infection', 'NOUN'),\n",
       " ('respiratory', 'NOUN'),\n",
       " ('symptoms', 'NOUN'),\n",
       " ('cough', 'NOUN'),\n",
       " ('shortness', 'NOUN'),\n",
       " ('breath', 'NOUN'),\n",
       " ('breathing', 'NOUN'),\n",
       " ('difficulties', 'NOUN'),\n",
       " ('cases', 'NOUN'),\n",
       " ('infection', 'NOUN'),\n",
       " ('pneumonia', 'NOUN'),\n",
       " ('respiratory', 'NOUN'),\n",
       " ('syndrome', 'NOUN'),\n",
       " ('kidney', 'NOUN'),\n",
       " ('failure', 'NOUN'),\n",
       " ('death', 'NOUN')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd=nltk.FreqDist([word[0] for word in text_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humans', 4), ('Coronaviruses', 3), ('Respiratory', 2)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd=nltk.ConditionalFreqDist((len(word[0]),word[0]) for word in text_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('CoV', 1)],\n",
       " [('cold', 1)],\n",
       " [('Acute', 1)],\n",
       " [('humans', 4)],\n",
       " [('animals', 2)]]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cfd[i].most_common(1) for i in range(3,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Acute', 1),\n",
       " ('civet', 1),\n",
       " ('signs', 1),\n",
       " ('cough', 1),\n",
       " ('cases', 1),\n",
       " ('death', 1)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[5].most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=5>Exercise 1</font>\n",
    "\n",
    "As we know, often the same word may be used as a different part of speech. \n",
    "\n",
    "  Find all the parts of speech used for the words <b>well, like</b> and <b>out</b>, of any case (upper or lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='red'>Exercise 2</font>\n",
    "\n",
    "In the text Alice:\n",
    "\n",
    "<ul>\n",
    "    <li>Find all the cases where there was a choice between two nouns. For example, <b> water</b> or <b>food</b></li>\n",
    "    <li> Find all the cases where there is a noun followed by the word <b>and</b> and another noun. For example, <b>apple</b> and <b>sword</b></li>\n",
    "  </ul>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Chunking</h2>\n",
    "\n",
    "Often times words come in pairs, for example <b> New Orleans, Coffee Shop, Star Wars, Coffee Table, TV Stand </b> etc. so we don't want to tokenize them separately, but rather we would want to keep them together so as not to loose meaning. We can do so via chunking, by specifying the type of word structures we'd like to chunk together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"We are taking a Data Science course at the Military Academy in the state of New York. We brought five\n",
    "coffee tables via two separate jet planes from New Orleans.We ran into cbs news station\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tag and tokenize to see what kind of tags our words of interest have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tag=nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('taking', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('Data', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " ('course', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Military', 'NNP'),\n",
       " ('Academy', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('state', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('brought', 'VBD'),\n",
       " ('five', 'CD'),\n",
       " ('coffee', 'NN'),\n",
       " ('tables', 'NNS'),\n",
       " ('via', 'IN'),\n",
       " ('two', 'CD'),\n",
       " ('separate', 'JJ'),\n",
       " ('jet', 'NN'),\n",
       " ('planes', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('Orleans.We', 'NNP'),\n",
       " ('ran', 'VBD'),\n",
       " ('into', 'IN'),\n",
       " ('cbs', 'JJ'),\n",
       " ('news', 'NN'),\n",
       " ('station', 'NN')]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are looking for is a noun followed by a noun without anything in between separating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=\"\"\"\n",
    "    chunk:\n",
    "    {<NN>+}\n",
    "    {<NNP>+}\n",
    "    {<NNPS>+}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk=nltk.RegexpParser(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=chunk.parse(text_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  are/VBP\n",
      "  taking/VBG\n",
      "  a/DT\n",
      "  (chunk Data/NNP Science/NNP)\n",
      "  (chunk course/NN)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (chunk Military/NNP Academy/NNP)\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (chunk state/NN)\n",
      "  of/IN\n",
      "  (chunk New/NNP York/NNP)\n",
      "  ./.\n",
      "  We/PRP\n",
      "  brought/VBD\n",
      "  five/CD\n",
      "  (chunk coffee/NN)\n",
      "  tables/NNS\n",
      "  via/IN\n",
      "  two/CD\n",
      "  separate/JJ\n",
      "  (chunk jet/NN)\n",
      "  planes/NNS\n",
      "  from/IN\n",
      "  (chunk New/NNP Orleans.We/NNP)\n",
      "  ran/VBD\n",
      "  into/IN\n",
      "  cbs/JJ\n",
      "  (chunk news/NN station/NN))\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stop Words</h2>\n",
    "\n",
    "When trying to extract only the meaningful words from a text, we want to get rid of what's known as non-descriptive or stop-words, such as `the, a, an, is ` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\valmir.bucaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=5>Exercise</font>\n",
    "\n",
    "Extract the top 10 most common words in the Alice book, that are descriptive to the text itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
